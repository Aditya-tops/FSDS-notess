RUNNING NOTES: 13 - MAR - 2025
--------------------------------

1. PREVIOUS TOPIC			-	ML: SIMPLE LINEAR REGRESSION
2. CURRENT TOPIC			-	ML: MULTIPLE LINEAR REGRESSION
3. UPCOMING TOPIC			-	ML: POLYNOMIAL MODEL
									         

----------------------------------------------------------------


INDEX
---------

0. DATA SCIENCE DEMO			-	Done

1. DATA SCIENCE FUNDAMENTALS		-	Done

--------------------------------------------------

PYTHON PROGRAMMING LANG
-----------------------

0. PYTHON - INSTALLATION		-	Done

1. PYTHON - INTRODUCTION		-	Done
2. PYTHON - KEYWORDS			-	Done
3. PYTHON - HELLO WORLD PROGRAM		-	Done
4. PYTHON - NAMING CONVENTIONS		-	Done
5. PYTHON - VARIABLES			-	Done
6. PYTHON - DATA TYPES			-	Done
7. PYTHON - OPERATORS			-	Done
8. PYTHON - INPUT & OUTPUT		-	Done
9. PYTHON - FLOW CONTROL		-	Done
10. PYTHON - STRING			-	Done
11. PYTHON - FUNCTIONS - PART - 1	-	Done
12. PYTHON - FUNCTIONS - PART - 2	-	Done
13. PYTHON - MODULE			-	Done
14. PYTHON - PACKAGE			-	Done
15. PYTHON - LIST DATA STRUCUTRE	-	Done
16. PYTHON - TUPLE DATA STRUCUTRE	-	Done
17. PYTHON - SET DATA STRUCUTRE		-	Done
18. PYTHON - DICTIONARY DATA STRUCUTRE	-	Done
19. PYTHON - OBJECT ORIENTED 		-	Done
	PROGRAMMING	

--------------------------------------------------

DATA ANALYSIS				
-------------

1. PANDAS - INTRODUCTION		-	Done
2. PANDAS - SERIES - INTRODUCTION	-	Done
3. PANDAS - NAN VALUE			-	Done
4. PANDAS - SERIES - ATTRIBUTES		-	Done
5. PANDAS - SERIES - METHODS		-	Done
6. PANDAS - DATAFRAME INTRODUCTION	-	Done
7. PANDAS - DATAFRAME - LOADING 	-	Done
	DIFFERENT FILES

8. PANDAS - DATAFRAME - ATTRIBUTES	-	Done
9. PANDAS - DATAFRAME - METHODS		-	Done

10. PANDAS - DATAFRAME - RENAMING 	-	Done
	COLUMN, INDEX

11. PANDAS - DATAFRAME - INPLACE 	-	Done
	PARAMETER

12. PANDAS -DATAFRAME - HANDLING 	-	Done
	MISSING NAN VALUES

13. PANDAS - DATAFRAME - SELECTION 	- 	Done
	LOC, ILOC

14. PANDAS - DATAFRAME - FILTERING	-	Done

15. PANDAS - DATAFRAME - SORTING	-	Done

16. PANDAS - DATAFRAME - GROUPBY	-	Done

17. PANDAS - DATAFRAME - MERGING 	-	Done
	OR JOINING

18. PANDAS - DATAFRAME - CONCAT		-	Done

19. PANDAS - DATAFRAME - ADDING, 	-	Done
	DROPPING ROWS AND COLUMNS

20. PANDAS - DATAFRAME - DATE AND 	-	Done
	TIME OPERATIONS

21. PANDAS - DATAFRAME - CONCATENATING	-	Done 
	MULTIPLE CSV FILES

--------------------------------------------------

DATA ANALYSIS PROJECT
---------------------

1. EDA PROJECT				-	Done

--------------------------------------------------

DATA VISUALIZATION
------------------

1. DATA VISUALIZATION PART 1		-	Done
2. DATA VISUALIZATION PART 2		-	Done
3. DATA VISUALIZATION FUNDAMENTALS	-	Done

4. DATA VISUALIZATION POWER BI		-	Upcoming topic	
		
--------------------------------------------------

NUMPY
-----

1. NUMPY INTRODUCTION			-	Done
2. NUMPY FUNDAMENTALS			-	Done
3. NUMPY ATTRIBUTES			-	Done
4. NUMPY METHODS			-	Done

--------------------------------------------------

MATHS						STATUS
-----						------

1. MATHS - PART - 1 - POPULATION, 	-	Done
	SAMPLE, TYPES OF VARIABLES


2. MATHS - PART - 2 - MODE, MEDIAN, 	-	Done
	MEAN, RANGE, 
	STANDARD DEVIATION, VARIANCE	


3. MATHS - PART - 3 - OUTLIERS		-	Done


4. MATHS - PART - 4 - THE FIVE NUMBERS	-	Done
	SUMMARY, BOX PLOT, OUTLIER


5. MATHS - PART - 5 - SYMMETRY AND 	-	Done
	SKEWNESS


6. MATHS - PART - 6 - EXPLANATORY AND 	-	Done
	RESPONSIVE VARIABLES


7. MATHS - PART - 7 - REGRESSION 	-	Done
	AND R SQUARED


8. MATHS - PART - 8 - RESIDUALS		-	Done


9. MATHS - PART - 9 - THE NORMAL 	-	Hold
	DISTRIBUTION AND 
	68-95-99.7 RULE

10. MATHS - PART - 10 - MATRIX		-	Done

--------------------------------------------------

FEATURE ENGINEERING
-------------------

1. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 1	-	Done


2. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 2	-	Done

--------------------------------------------------

MACHINE LEARNING
----------------

1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	INTRODUCTION

2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TERMINOLOGY

3. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DATA AND ML ALGORITHMS

4. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LEARNING FUNCTION

5. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TYPES OF MODELS

6. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LIFE CYCLE

7. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TRAIN & TEST DATASETS


8. DATA SCIENCE - MACHINE LEARNING 	- 	Upcoming topic	
	R VALUE


9. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION

9.1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

9.2. DATA SCIENCE - MACHINE LEARNING 	- 	Share
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	MULTIPLE LINEAR REGRESSION


12. DATA SCIENCE - MACHINE LEARNING 	- 	Running topic
	PICKLING AND UNPICKLING


13. DATA SCIENCE - MACHINE LEARNING 	- 	Upcoming topic
	SAVE MODEL USING JOBLIB AND 
	PICKLING


--------------------------------------------------

Predefined functions
---------------------

1. print(p)		->	To display the output
2. type(p)		->	To check the data type
3. range(p)		->	To get range of values
4. input(p)		->	To take valut at runtime/dynamically
5. len(p)		->	To find number of values in sequence

6. float(p)		->	To convert to float
7. int(p)		->	To convert to int
8. list(p)		->	Convert from seq to list
9. tuple(p)		->	Convert from seq to tuple
10. set(p)		->	Convert from seq to set

11. dict(p)		->	Convert from list of tups to dict

--------------------------------

Errors
------

1. SyntaxError
2. NameError
3. KeyError
4. ValueError
5. TypeError

6. IndexError
7. IndentationError
8. AttributeError
9. ModuleNotFoundError
10. FileNotFoundError

11. InvalidParameterError

--------------------------------

10. DATA SCIENCE - MACHINE LEARNING 
	MULTIPLE LINEAR REGRESSION

--------------------------------

ML flow
-------

Data
	DataFrame
		Feature Engg
				Array
					Machine Learning Algorithm
					Cost function
					Gradient Descent Algorithm
						Increase accuracy
						Reduce Error
							Bias
							Variance


ML steps
--------

	1. Importing the libraries
	2. Loading the dataset
	3. Data preparation
	4. Splitting the dataset
	5. Model creation
	6. Model training
	7. Prediction

--------------------------------

Tech AI Update

China		->	DeepSeek R1
China		->	Manus
		->	https://manus.im/

--------------------------------

Regression

	1. Simple Linear Regression
	2. Multiple Linear Regression
	3. Polynomial Regression
	4. Lasso Regression
	5. Ridge Regression

--------------------------------

2. Multiple Linear Regression

	More features 	+ 	one output
	3 features	+	one output

--------------------------------

Simple Linear Regression
					class

1. One feature + One target		LinearRegression
2. Three features + One target		LinearRegression

--------------------------------

In python, e represents?

In python we can create floating values in two ways

	1. 	200.0	->	floating value
	2. 	2e2	->	floating value


		      2
2e2	=	2 * 10
	=	2 * 10 * 10
	=	200.0

--------------------------------

a = 2e2

print(a) # 200.0

--------------------------------

# Steps from 1 to 6

print("Topic: Multiple Linear Regression")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.linear_model import LinearRegression




print("Step 2: Loading the dataset")

df = pd.read_csv("homeprices1.csv")

m = df.bedrooms.median()
df.bedrooms = df.bedrooms.fillna(m)




print("Step 3: Data preparation")

a = df.drop('price', axis = 'columns')

X = a.values
y = df["price"].values




print("Step 4: Splitting dataset: Optional")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training")

model.fit(X, y)

--------------------------------

from Masud Siddiqi (privately):    6:14 PM
Showing the dataset at the beginning of coding helps refresh the konwledge of features and lables.

Coin			=	H	+	T

Data Science proj	=  Data Analysis+ 	model creation

			=	pandas	+	ml


--------------------------------

model.fit(below input, output)

area	bedrooms	age

2600	3		20
3000	4		15
3200	4		18
3600	3		30
4000	5		8
4100	6		8

While doing prediction: follow the order

model.predict([[area, bedrooms, age]])

model.predict([[3, 3300, 20]])

--------------------------------

IMP into!!!
-----------

In All ML algorithms

	1. fit() method is for train the model
	2. predict() method is for prediction


--------------------------------

# Capture the coef and intercept values

print("Topic: Multiple Linear Regression")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.linear_model import LinearRegression




print("Step 2: Loading the dataset")

df = pd.read_csv("homeprices1.csv")

m = df.bedrooms.median()
df.bedrooms = df.bedrooms.fillna(m)




print("Step 3: Data preparation")

a = df.drop('price', axis = 'columns')

X = a.values
y = df["price"].values




print("Step 4: Splitting dataset: Optional")


print("Step 5: Model creation")

model = LinearRegression()



print("Step 6: Model training")

model.fit(X, y)

print()
print(model.intercept_)

print()
print(model.coef_)


Output
------

intercept:	221323.0018654043

coefitient values: [  112.06244194 23388.88007794 -3231.71790863]

--------------------------------

# Steps from 1 to 7

print("Topic: Multiple Linear Regression")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.linear_model import LinearRegression




print("Step 2: Loading the dataset")

df = pd.read_csv("homeprices1.csv")

m = df.bedrooms.median()
df.bedrooms = df.bedrooms.fillna(m)




print("Step 3: Data preparation")

a = df.drop('price', axis = 'columns')

X = a.values
y = df["price"].values




print("Step 4: Splitting dataset: Optional")


print("Step 5: Model creation")

model = LinearRegression()



print("Step 6: Model training")

model.fit(X, y)




print("Step 7: Prediction")

print()
print(model.predict([[3000, 3, 40]]))

--------------------------------

Play with little maths
-----------------------

intercept:	221323.0018654043

coefitient values: [  112.06244194 23388.88007794 -3231.71790863]

-----------------------

model.predict([[3000, 3, 40]])

area		->	3000
bedrooms	->	3
age		->	40

Price_Tag = Area * c1 + BedRooms * c2 + Age * c3 + intercept

p_t = 3000 * 112.06244194 + 3 * 23388.88007794 + 40 * -3231.71790863 + 221323.0018654043

--------------------------------

# predict() method and math calculations

print("Topic: Multiple Linear Regression")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.linear_model import LinearRegression




print("Step 2: Loading the dataset")

df = pd.read_csv("homeprices1.csv")

m = df.bedrooms.median()
df.bedrooms = df.bedrooms.fillna(m)




print("Step 3: Data preparation")

a = df.drop('price', axis = 'columns')

X = a.values
y = df["price"].values




print("Step 4: Splitting dataset: Optional")


print("Step 5: Model creation")

model = LinearRegression()



print("Step 6: Model training")

model.fit(X, y)




print("Step 7: Prediction")

print()
print(model.predict([[3000, 3, 40]]))

p_t = 3000 * 112.06244194 + 3 * 23388.88007794 + 40 * -3231.71790863 + 221323.0018654043

print()
print(p_t)

--------------------------------

12. DATA SCIENCE - MACHINE LEARNING 	
	PICKLING AND UNPICKLING

--------------------------------

Pickling and Unpickling

1. This topic belongs to Python: File IO
2. We are applying in Machine learning

--------------------------------

Pickling
--------

	-> Writing object information into a file


Unpickling
----------

	-> Reading object information from a file

--------------------------------

Realtime Example
----------------

Prasad	Banglore


Daniel	Hyderabad

----------------

Program
	class Student
		constructor
			instance variables
		method
	object

	run: demo1.py
	output


Daniel:	10 mins:	Task done
			demo1.py


Daniel				Prasad
demo1.py	send		?


W1	Bus ticket		
W2	Flight ticket		
W3	send immediately
	Gmail
		uploaded: demo1.py
		
	send

	Task!!!


	It is not a good practice to share the file over gmail
	Best option
		apply pickling is best the option


--------------------------------

Coming Saturday:

No exam:	

Pandas:		dataset1, dataset2

use dataset1	implement examples

takes sales5.csv	implement all groupby scenarios



--------------------------------

Daily
-----

1. Running notes				->	Sharing
2. Materials (PDF format)			->	Sharing

We are sharing by using 			->	Google classroom

--------------------------------
