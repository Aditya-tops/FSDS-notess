RUNNING NOTES: 25 - APRIL - 2025
--------------------------------

1. PREVIOUS TOPIC			-	DL: MODEL EVALUATION
2. CURRENT TOPIC			-	DL: MODEL EVALUATION CONTINU...
3. UPCOMING TOPIC			-	DL: MODEL SAVING

----------------------------------------------------------------


INDEX
---------

0. DATA SCIENCE DEMO			-	Done

1. DATA SCIENCE FUNDAMENTALS		-	Done

--------------------------------------------------

PYTHON PROGRAMMING LANG
-----------------------

0. PYTHON - INSTALLATION		-	Done

1. PYTHON - INTRODUCTION		-	Done
2. PYTHON - KEYWORDS			-	Done
3. PYTHON - HELLO WORLD PROGRAM		-	Done
4. PYTHON - NAMING CONVENTIONS		-	Done
5. PYTHON - VARIABLES			-	Done
6. PYTHON - DATA TYPES			-	Done
7. PYTHON - OPERATORS			-	Done
8. PYTHON - INPUT & OUTPUT		-	Done
9. PYTHON - FLOW CONTROL		-	Done
10. PYTHON - STRING			-	Done
11. PYTHON - FUNCTIONS - PART - 1	-	Done
12. PYTHON - FUNCTIONS - PART - 2	-	Done
13. PYTHON - MODULE			-	Done
14. PYTHON - PACKAGE			-	Done
15. PYTHON - LIST DATA STRUCUTRE	-	Done
16. PYTHON - TUPLE DATA STRUCUTRE	-	Done
17. PYTHON - SET DATA STRUCUTRE		-	Done
18. PYTHON - DICTIONARY DATA STRUCUTRE	-	Done
19. PYTHON - OBJECT ORIENTED 		-	Done
	PROGRAMMING	

--------------------------------------------------

DATA ANALYSIS				
-------------

1. PANDAS - INTRODUCTION		-	Done
2. PANDAS - SERIES - INTRODUCTION	-	Done
3. PANDAS - NAN VALUE			-	Done
4. PANDAS - SERIES - ATTRIBUTES		-	Done
5. PANDAS - SERIES - METHODS		-	Done
6. PANDAS - DATAFRAME INTRODUCTION	-	Done
7. PANDAS - DATAFRAME - LOADING 	-	Done
	DIFFERENT FILES

8. PANDAS - DATAFRAME - ATTRIBUTES	-	Done
9. PANDAS - DATAFRAME - METHODS		-	Done

10. PANDAS - DATAFRAME - RENAMING 	-	Done
	COLUMN, INDEX

11. PANDAS - DATAFRAME - INPLACE 	-	Done
	PARAMETER

12. PANDAS -DATAFRAME - HANDLING 	-	Done
	MISSING NAN VALUES

13. PANDAS - DATAFRAME - SELECTION 	- 	Done
	LOC, ILOC

14. PANDAS - DATAFRAME - FILTERING	-	Done

15. PANDAS - DATAFRAME - SORTING	-	Done

16. PANDAS - DATAFRAME - GROUPBY	-	Done

17. PANDAS - DATAFRAME - MERGING 	-	Done
	OR JOINING

18. PANDAS - DATAFRAME - CONCAT		-	Done

19. PANDAS - DATAFRAME - ADDING, 	-	Done
	DROPPING ROWS AND COLUMNS

20. PANDAS - DATAFRAME - DATE AND 	-	Done
	TIME OPERATIONS

21. PANDAS - DATAFRAME - CONCATENATING	-	Done 
	MULTIPLE CSV FILES

--------------------------------------------------

DATA ANALYSIS PROJECT
---------------------

1. EDA PROJECT				-	Done

--------------------------------------------------

DATA VISUALIZATION
------------------

1. DATA VISUALIZATION PART 1		-	Done
2. DATA VISUALIZATION PART 2		-	Done
3. DATA VISUALIZATION FUNDAMENTALS	-	Done

4. DATA VISUALIZATION POWER BI		-	Upcoming topic	
		
--------------------------------------------------

NUMPY
-----

1. NUMPY INTRODUCTION			-	Done
2. NUMPY FUNDAMENTALS			-	Done
3. NUMPY ATTRIBUTES			-	Done
4. NUMPY METHODS			-	Done

--------------------------------------------------

MATHS						STATUS
-----						------

1. MATHS - PART - 1 - POPULATION, 	-	Done
	SAMPLE, TYPES OF VARIABLES


2. MATHS - PART - 2 - MODE, MEDIAN, 	-	Done
	MEAN, RANGE, 
	STANDARD DEVIATION, VARIANCE	


3. MATHS - PART - 3 - OUTLIERS		-	Done


4. MATHS - PART - 4 - THE FIVE NUMBERS	-	Done
	SUMMARY, BOX PLOT, OUTLIER


5. MATHS - PART - 5 - SYMMETRY AND 	-	Done
	SKEWNESS


6. MATHS - PART - 6 - EXPLANATORY AND 	-	Done
	RESPONSIVE VARIABLES


7. MATHS - PART - 7 - REGRESSION 	-	Done
	AND R SQUARED


8. MATHS - PART - 8 - RESIDUALS		-	Done


9. MATHS - PART - 9 - THE NORMAL 	-	Hold
	DISTRIBUTION AND 
	68-95-99.7 RULE

10. MATHS - PART - 10 - MATRIX		-	Done

--------------------------------------------------

FEATURE ENGINEERING
-------------------

1. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 1	-	Done


2. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 2	-	Done

--------------------------------------------------

MACHINE LEARNING
----------------

1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	INTRODUCTION

2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TERMINOLOGY

3. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DATA AND ML ALGORITHMS

4. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LEARNING FUNCTION

5. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TYPES OF MODELS

6. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LIFE CYCLE

7. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TRAIN & TEST DATASETS


8. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	R VALUE


9. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION

9.1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

9.2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	MULTIPLE LINEAR REGRESSION


11. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	PICKLING AND UNPICKLING


12. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SAVE MODEL USING JOBLIB AND 
	PICKLING


13. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	POLYNOMIAL FEATURES

14. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DUMMY VARIABLE, ONEHOTENCODING


15. DATA SCIENCE - MACHINE LEARNING 	-	Done
	- R VALUE


16. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	COST FUNCTION


17. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	REGRESSION COST FUNCTION


18. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	LOGISTIC REGRESSION
	BINARY CLASSIFICATION

19. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LOGISTIC REGRESSION
	MULTI CLASS CLASSIFICATION


20. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DECISION TREE


21. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	RANDOM FOREST ALGORITHM



22. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	K-FOLD CROSS VALIDATION


23. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SVM

24. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	GRADIENT DESCENT ALGORITHM


25. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	UNDERFITTING & OVERFITTING


26. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	BIAS - VARIANCE TRADE OFF


27. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	K - MEANS CLUSTERING


28. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	HYPER PARAMETER TUNING 
	GRIDSEARCH CV


29. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	K NEAREST NEIGHBOR


30. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	XGBOOST

31. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	NAIVE BAYES CLASSIFIER


32. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	CONFUSION MATRIX


33. DATA SCIENCE - MACHINE LEARNING 	- 	Upcoming topic
	LASSO & RIDGE REGRESSION	

--------------------------------------------------

DEEP LEARNING
-------------

1. DEEP LEARNING - INTRODUCTION		-	Done
2. DEEP LEARNING - LIBRARIES		-	Done

3. DEEP LEARNING - IMPORTANT 		-	Done
	TERMINOLOGY

4. DEEP LEARNING - MULTILAYER 		-	Done
	PERCEPTRONS

5. DEEP LEARNING - MULTILAYER 		-	Done
	PERCEPTRONS EXAMPLE	

6. DEEP LEARNING - EVALUATE MODEL 	-	Done
	PERFORMANCE	

7. DEEP LEARNING - EVALUATE MODEL 	-	Done
	SAVE MODEL

8. DEEP LEARNING - BEST MODEL CHECK 	-	Running topic
	POINT


--------------------------------------------------

Predefined functions
---------------------

1. print(p)		->	To display the output
2. type(p)		->	To check the data type
3. range(p)		->	To get range of values
4. input(p)		->	To take valut at runtime/dynamically
5. len(p)		->	To find number of values in sequence

6. float(p)		->	To convert to float
7. int(p)		->	To convert to int
8. list(p)		->	Convert from seq to list
9. tuple(p)		->	Convert from seq to tuple
10. set(p)		->	Convert from seq to set

11. dict(p)		->	Convert from list of tups to dict

--------------------------------

Errors
------

1. SyntaxError
2. NameError
3. KeyError
4. ValueError
5. TypeError

6. IndexError
7. IndentationError
8. AttributeError
9. ModuleNotFoundError
10. FileNotFoundError

11. InvalidParameterError

------------------------------------

6. DEEP LEARNING - EVALUATE MODEL 
	PERFORMANCE
--------------------------------

Deep learning Steps
-------------------

	1. Importing the libraries
	2. Loading the dataset
	3. Data preparation
	4. Splitting the dataset

	5. Model creation		model = Sequential()	
	6. Model compillation		model.compile(...)
	7. Model training		model.fit(...)
	8. Prediction			model.predict(...)

--------------------------------

Complete Deep Learning Flow

inputs
	weights
		Bias
			summation function
				activation function
					cost function
						optimization fun
							output


--------------------------------

IMP points

-> How to create Deep Learning Model(Neural Network model)
-> Model performance

	1.1 Automatic
	1.2 Manual 

	2. K fold cross validation


--------------------------------

2. K fold cross validation

# To recognize Dense class in package

>>> import tensorflow.keras.layers as tkl
>>> dir(tkl)

[...'Dense', ...]

--------------------------------

# DL with K fold cross validation

print("Topic: DL, K fold cross validation")
print()


print("Step 1: Importing the libraries")

import numpy as np
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from sklearn.model_selection import StratifiedKFold


import warnings
warnings.filterwarnings("ignore")






print("Step 2: Loading the dataset")

dataset = np.loadtxt(
    'pima-indians-diabetes.csv', 
    delimiter = ','
)



print("Step 3: Data preparation")

X = dataset[:, 0:8]
y = dataset[:, 8]




print("Step 4: Splitting the dataset")

kfold = StratifiedKFold(
    n_splits = 10, 
    shuffle = True, 
    random_state = 7
)


cvscores = []

for train, test in kfold.split(X, y):
    
    model = Sequential()
    
    model.add(Dense(12, input_shape=(8,), activation='relu'))
    model.add(Dense(8, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    
    model.compile(
        loss = 'binary_crossentropy', 
        optimizer = 'adam', 
        metrics = ['accuracy']
    )
    
    model.fit(
        X[train], 
        y[train], 
        epochs = 150, 
        batch_size = 10, 
        verbose = 0
    )
    
    scores = model.evaluate(X[test], y[test])
    
    print(("Accuracy:", scores[1]*100))
    
    cvscores.append(scores[1] * 100)
    
print("Mean Accuracy", np.mean(cvscores))

--------------------------------

*** Saving the DL model ***

1. How to save ml model
	-> by using pickling and joblib

2. How to save dl model
	-> 1. Save model in json file
	-> 2. Save model weights in hdf5 format

--------------------------------

Small suggestion:
-----------------

You guys done only 1/2/3 time preparation

but you didnt reach the flow

Please do more revisions

--------------------------------

# Creating model, Save the model & Weights

print("Topic: DL, Save the DL model")
print()


print("Step 1: Importing the libraries")

from numpy import loadtxt

from keras.models import model_from_json
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential





print("Step 2: Loading the dataset")

dataset = loadtxt(
    'pima-indians-diabetes.csv', 
    delimiter = ','
)



print("Step 3: Data preparation")

X = dataset[:, 0:8]
y = dataset[:, 8]



print("Step 4: Splitting the dataset: Optional")




print("Step 5: Model creation")


model = Sequential()



print("Step 5.1: Creating a layers and adding to model")

model.add(Dense(12, input_shape = (8, ), activation = 'relu'))
model.add(Dense(8, activation = 'relu'))
model.add(Dense(1, activation = 'sigmoid'))




print("Step 6: Model compillation")

model.compile(
    loss = 'binary_crossentropy', 
    optimizer = 'adam', 
    metrics = ['accuracy']
)



print("Step 7: Model training")

model.fit(X, y, epochs = 150, batch_size = 10)



print("Step Spl1: Save the model & weights into json")

model_json = model.to_json()

with open("prasad.json", "w") as json_file:
    json_file.write(model_json)




model.save_weights("model.weights.h5")

print("Saved model to disk")
print("Done")

--------------------------------

# Load the model and weights
# Evaluate the model


import numpy as np
from keras.models import model_from_json


dataset = np.loadtxt(
    'pima-indians-diabetes.csv', 
    delimiter = ','
)


X = dataset[:, 0:8]
y = dataset[:, 8]




json_file = open('prasad.json' , 'r')
model_j = json_file.read()
model = model_from_json(model_j)

model.load_weights("model.weights.h5")

print("Loaded model from disk")


model.compile(
    loss = 'binary_crossentropy', 
    optimizer = 'rmsprop', 
    metrics=['accuracy']
)

score = model.evaluate(X, y)

print(score[1])

--------------------------------

8. DEEP LEARNING - BEST MODEL CHECK 	
	POINT

--------------------------------

Father example
--------------

DataSet	->	70:30 

i1	->	features (White, small)	->	Dog	1%
i2	->	features (White, small)	->	Dog	1%
i3	->	features (Black, small)	->	Dog	2%
i4	->	features (Brown, small)	->	Cat	4%
i5	->	features (Brown, small)	->	Cat	4%


--------------------------------

-> Let me continue tomorrow
-> We done today session
		 - Daniel

--------------------------------


Daily
-----

1. Running notes				->	Sharing
2. Materials (PDF format)			->	Sharing

We are sharing by using 			->	Google classroom

--------------------------------
