RUNNING NOTES: 02 - APRIL - 2025
--------------------------------

1. PREVIOUS TOPIC			-	ML: K-FOLD CROSS VALIDATION
2. CURRENT TOPIC			-	ML: SVM
3. UPCOMING TOPIC			-	ML: GRADIENT DESCENT ALGORITHM

----------------------------------------------------------------


INDEX
---------

0. DATA SCIENCE DEMO			-	Done

1. DATA SCIENCE FUNDAMENTALS		-	Done

--------------------------------------------------

PYTHON PROGRAMMING LANG
-----------------------

0. PYTHON - INSTALLATION		-	Done

1. PYTHON - INTRODUCTION		-	Done
2. PYTHON - KEYWORDS			-	Done
3. PYTHON - HELLO WORLD PROGRAM		-	Done
4. PYTHON - NAMING CONVENTIONS		-	Done
5. PYTHON - VARIABLES			-	Done
6. PYTHON - DATA TYPES			-	Done
7. PYTHON - OPERATORS			-	Done
8. PYTHON - INPUT & OUTPUT		-	Done
9. PYTHON - FLOW CONTROL		-	Done
10. PYTHON - STRING			-	Done
11. PYTHON - FUNCTIONS - PART - 1	-	Done
12. PYTHON - FUNCTIONS - PART - 2	-	Done
13. PYTHON - MODULE			-	Done
14. PYTHON - PACKAGE			-	Done
15. PYTHON - LIST DATA STRUCUTRE	-	Done
16. PYTHON - TUPLE DATA STRUCUTRE	-	Done
17. PYTHON - SET DATA STRUCUTRE		-	Done
18. PYTHON - DICTIONARY DATA STRUCUTRE	-	Done
19. PYTHON - OBJECT ORIENTED 		-	Done
	PROGRAMMING	

--------------------------------------------------

DATA ANALYSIS				
-------------

1. PANDAS - INTRODUCTION		-	Done
2. PANDAS - SERIES - INTRODUCTION	-	Done
3. PANDAS - NAN VALUE			-	Done
4. PANDAS - SERIES - ATTRIBUTES		-	Done
5. PANDAS - SERIES - METHODS		-	Done
6. PANDAS - DATAFRAME INTRODUCTION	-	Done
7. PANDAS - DATAFRAME - LOADING 	-	Done
	DIFFERENT FILES

8. PANDAS - DATAFRAME - ATTRIBUTES	-	Done
9. PANDAS - DATAFRAME - METHODS		-	Done

10. PANDAS - DATAFRAME - RENAMING 	-	Done
	COLUMN, INDEX

11. PANDAS - DATAFRAME - INPLACE 	-	Done
	PARAMETER

12. PANDAS -DATAFRAME - HANDLING 	-	Done
	MISSING NAN VALUES

13. PANDAS - DATAFRAME - SELECTION 	- 	Done
	LOC, ILOC

14. PANDAS - DATAFRAME - FILTERING	-	Done

15. PANDAS - DATAFRAME - SORTING	-	Done

16. PANDAS - DATAFRAME - GROUPBY	-	Done

17. PANDAS - DATAFRAME - MERGING 	-	Done
	OR JOINING

18. PANDAS - DATAFRAME - CONCAT		-	Done

19. PANDAS - DATAFRAME - ADDING, 	-	Done
	DROPPING ROWS AND COLUMNS

20. PANDAS - DATAFRAME - DATE AND 	-	Done
	TIME OPERATIONS

21. PANDAS - DATAFRAME - CONCATENATING	-	Done 
	MULTIPLE CSV FILES

--------------------------------------------------

DATA ANALYSIS PROJECT
---------------------

1. EDA PROJECT				-	Done

--------------------------------------------------

DATA VISUALIZATION
------------------

1. DATA VISUALIZATION PART 1		-	Done
2. DATA VISUALIZATION PART 2		-	Done
3. DATA VISUALIZATION FUNDAMENTALS	-	Done

4. DATA VISUALIZATION POWER BI		-	Upcoming topic	
		
--------------------------------------------------

NUMPY
-----

1. NUMPY INTRODUCTION			-	Done
2. NUMPY FUNDAMENTALS			-	Done
3. NUMPY ATTRIBUTES			-	Done
4. NUMPY METHODS			-	Done

--------------------------------------------------

MATHS						STATUS
-----						------

1. MATHS - PART - 1 - POPULATION, 	-	Done
	SAMPLE, TYPES OF VARIABLES


2. MATHS - PART - 2 - MODE, MEDIAN, 	-	Done
	MEAN, RANGE, 
	STANDARD DEVIATION, VARIANCE	


3. MATHS - PART - 3 - OUTLIERS		-	Done


4. MATHS - PART - 4 - THE FIVE NUMBERS	-	Done
	SUMMARY, BOX PLOT, OUTLIER


5. MATHS - PART - 5 - SYMMETRY AND 	-	Done
	SKEWNESS


6. MATHS - PART - 6 - EXPLANATORY AND 	-	Done
	RESPONSIVE VARIABLES


7. MATHS - PART - 7 - REGRESSION 	-	Done
	AND R SQUARED


8. MATHS - PART - 8 - RESIDUALS		-	Done


9. MATHS - PART - 9 - THE NORMAL 	-	Hold
	DISTRIBUTION AND 
	68-95-99.7 RULE

10. MATHS - PART - 10 - MATRIX		-	Done

--------------------------------------------------

FEATURE ENGINEERING
-------------------

1. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 1	-	Done


2. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 2	-	Done

--------------------------------------------------

MACHINE LEARNING
----------------

1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	INTRODUCTION

2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TERMINOLOGY

3. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DATA AND ML ALGORITHMS

4. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LEARNING FUNCTION

5. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TYPES OF MODELS

6. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LIFE CYCLE

7. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TRAIN & TEST DATASETS


8. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	R VALUE


9. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION

9.1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

9.2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	MULTIPLE LINEAR REGRESSION


12. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	PICKLING AND UNPICKLING


13. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SAVE MODEL USING JOBLIB AND 
	PICKLING


10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	POLYNOMIAL FEATURES

11. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DUMMY VARIABLE, ONEHOTENCODING


12. DATA SCIENCE - MACHINE LEARNING 	-	Done
	- R VALUE


13. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	COST FUNCTION


14. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	REGRESSION COST FUNCTION


15. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	LOGISTIC REGRESSION
	BINARY CLASSIFICATION

16. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LOGISTIC REGRESSION
	MULTI CLASS CLASSIFICATION


17. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DECISION TREE


18. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	RANDOM FOREST ALGORITHM



24. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	K-FOLD CROSS VALIDATION


25. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SVM

26. DATA SCIENCE - MACHINE LEARNING 	- 	Running topic
	GRADIENT DESCENT ALGORITHM

27. NEW ALGORITHM


--------------------------------------------------

Predefined functions
---------------------

1. print(p)		->	To display the output
2. type(p)		->	To check the data type
3. range(p)		->	To get range of values
4. input(p)		->	To take valut at runtime/dynamically
5. len(p)		->	To find number of values in sequence

6. float(p)		->	To convert to float
7. int(p)		->	To convert to int
8. list(p)		->	Convert from seq to list
9. tuple(p)		->	Convert from seq to tuple
10. set(p)		->	Convert from seq to set

11. dict(p)		->	Convert from list of tups to dict

--------------------------------

Errors
------

1. SyntaxError
2. NameError
3. KeyError
4. ValueError
5. TypeError

6. IndexError
7. IndentationError
8. AttributeError
9. ModuleNotFoundError
10. FileNotFoundError

11. InvalidParameterError

------------------------------------

25. DATA SCIENCE - MACHINE LEARNING 	
	SVM

------------------------------------

Imp ponits!!!
-------------

	1. ML Flow
	2. ML Steps

------------------------------------

ML flow
-------

Data
	DataFrame
		Feature Engg
				Array
					Machine Learning Algorithm
					Cost function
					Gradient Descent Algorithm
						Increase accuracy
						Reduce Error
							Bias
							Variance


ML steps
--------

	1. Importing the libraries
	2. Loading the dataset
	3. Data preparation
	4. Splitting the dataset
	5. Model creation
	6. Model training
	7. Prediction

--------------------------------

SVM
---
	1. Regression		->	Support Vector Regressor
	2. Classification	->	Support Vector Classifier

--------------------------------

# Plotting setosa and versicolor flowers data
# Sepal length and Sepal width

print("Topic: SVM")
print()



print("Step 1: Importing the libraries")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris





print("Step 2: Loading the dataset")

iris = load_iris()





print("Step 3: Data preparation: X, y")

df = pd.DataFrame(
    iris.data, 
    columns = iris.feature_names
)

df['target'] = iris.target
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])    

df0 = df[:50]
df1 = df[50:100]
df2 = df[100:]




print("Step Spl: Data Viz")

plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')

plt.scatter(
    df0['sepal length (cm)'], 
    df0['sepal width (cm)'], 
    color = "green", 
    marker = '*'
)



plt.scatter(
    df1['sepal length (cm)'], 
    df1['sepal width (cm)'], 
    color = "blue", 
    marker = '.'
)


plt.show()

--------------------------------

# Plotting setosa and versicolor flowers data
# Petal length and Petal width

print("Topic: SVM")
print()



print("Step 1: Importing the libraries")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris





print("Step 2: Loading the dataset")

iris = load_iris()





print("Step 3: Data preparation: X, y")

df = pd.DataFrame(
    iris.data, 
    columns = iris.feature_names
)

df['target'] = iris.target
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])    

df0 = df[:50]
df1 = df[50:100]
df2 = df[100:]




print("Step Spl: Data Viz")

plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

plt.scatter(
    df0['petal length (cm)'], 
    df0['petal width (cm)'], 
    color = "green", 
    marker = '*'
)


plt.scatter(
    df1['petal length (cm)'], 
    df1['petal width (cm)'], 
    color = "blue", 
    marker = '.'
)


plt.show()

--------------------------------

# Checking model score

print("Topic: SVM")
print()



print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split





print("Step 2: Loading the dataset")

iris = load_iris()





print("Step 3: Data preparation: X, y")

df = pd.DataFrame(
    iris.data, 
    columns = iris.feature_names
)

df['target'] = iris.target
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])    

X = df.drop(['target', 'flower_name'], axis = 'columns')
y = df.target




print("Step 4: Splitting the dataset")

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size = 0.2
)





print("Step 5: Model creation")

model = SVC()




print("Step 6: Model training")

model.fit(X_train, y_train)




print("Step Spl: Checking score")

print()
print(model.score(X_test, y_test))

--------------------------------

from Masud Siddiqi (privately):    6:03 PM
There so many details in Data Analysis and ML. Do interviewers ask these details?

Good question: Definitely they will ask

How to ans: Simple follow Daniel's materials


--------------------------------

from Masud Siddiqi (privately):    6:06 PM
the predict () function accept variable length arguments?

Ans: We need to check the documenation, as of now we can proceed with input values

Sir, Let me correct your terminology:

predict() is method, its not a function

--------------------------------

# Steps from 1 to 7

print("Topic: SVM")
print()



print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split





print("Step 2: Loading the dataset")

iris = load_iris()





print("Step 3: Data preparation: X, y")

df = pd.DataFrame(
    iris.data, 
    columns = iris.feature_names
)

df['target'] = iris.target
df['flower_name'] = df.target.apply(lambda x: iris.target_names[x])    

X = df.drop(['target', 'flower_name'], axis = 'columns').values
y = df.target.values




print("Step 4: Splitting the dataset")

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size = 0.2
)





print("Step 5: Model creation")

model = SVC()




print("Step 6: Model training")

model.fit(X_train, y_train)




print("Step 7: Prediction")


print()
print(model.predict([[4.8,3.0,1.5,0.3]]))

--------------------------------

26. DATA SCIENCE - MACHINE LEARNING 	
	GRADIENT DESCENT ALGORITHM

--------------------------------

GRADIENT DESCENT ALGORITHM:

ML flow
-------

Data
	DataFrame
		Feature Engg
				Array
					Machine Learning Algorithm
					Cost function
					Gradient Descent Algorithm
						Increase accuracy
						Reduce Error
							Bias
							Variance


--------------------------------

from Masud Siddiqi (privately):    6:22 PM
Model gains the knowledge. Here knowledge mean the algorithm or data?

Father		->	Jeswanth	Data Scientist
Son		->	Daniel		Algorithm
Images		->	100[70:30] == 	knowledge

During train	->	Gani knowledge from the Data

--------------------------------

Gradient Descent Algorithm
--------------------------

1. Parameters initialize with (coe = 0, inter = 0)
2. Prediction with parameters
3. Cost function: Actual val - Prediction
4. Checking Error
	if Error is small
		Stop training
	if Error is large
		update Parameters (coe = 1, inter = 1)
		Prediction with parameters
		Cost function: Actual val - Prediction 
		Checking Error
			if Error is small
				Stop training
			if Error is large
				update parameters(coe = 2, inter = 2)
				prediction
				cost function
				Error
					small
						stop training
					large
						keep on.....


--------------------------------

from Masud Siddiqi (privately):    6:33 PM
If the fit() internally applies gradient descent algo to minimize the error/cost (gap between actual and prediction) by updating the coefficient (slope) and intercept, why do we need to bother studying this algo explicitly?

Point 1: It would be good to know backend process

Point 2: You know what, In Deep learning, we can customize gradient descent algorithm


--------------------------------

from peeyush sahu (privately):    6:38 PM
sir , how to decide how much error is tolerable and stop training?

Great question sir jiii

-> Gradient Descent Alg will take care

-> I will show you(everyone) like, how to see the error(During Bias and Variance)

--------------------------------

Gradient Descent Alg
--------------------

1. Initialize the parameters(c = 0, i = 0), random values
2. Prediction
3. Cost function
4. Error
	small
		stop training
	large
		update the parameters
		Prediction
		Cost function
		Error
			small
				stop training
			large
				update the parameters
				Cinema goes on....
				untill to reach small error

		Done

--------------------------------

3. Convergence

-> I will continue tomorrow
		   - Daniel


--------------------------------

Daily
-----

1. Running notes				->	Sharing
2. Materials (PDF format)			->	Sharing

We are sharing by using 			->	Google classroom

--------------------------------
