RUNNING NOTES: 07 - APRIL - 2025
--------------------------------

1. PREVIOUS TOPIC			-	ML: BIAS & VARIANCE TRADE OFF
2. CURRENT TOPIC			-	ML: K MEANS CLUSTERING
3. UPCOMING TOPIC			-	ML: HYPER PARAMETER TUNING

----------------------------------------------------------------


INDEX
---------

0. DATA SCIENCE DEMO			-	Done

1. DATA SCIENCE FUNDAMENTALS		-	Done

--------------------------------------------------

PYTHON PROGRAMMING LANG
-----------------------

0. PYTHON - INSTALLATION		-	Done

1. PYTHON - INTRODUCTION		-	Done
2. PYTHON - KEYWORDS			-	Done
3. PYTHON - HELLO WORLD PROGRAM		-	Done
4. PYTHON - NAMING CONVENTIONS		-	Done
5. PYTHON - VARIABLES			-	Done
6. PYTHON - DATA TYPES			-	Done
7. PYTHON - OPERATORS			-	Done
8. PYTHON - INPUT & OUTPUT		-	Done
9. PYTHON - FLOW CONTROL		-	Done
10. PYTHON - STRING			-	Done
11. PYTHON - FUNCTIONS - PART - 1	-	Done
12. PYTHON - FUNCTIONS - PART - 2	-	Done
13. PYTHON - MODULE			-	Done
14. PYTHON - PACKAGE			-	Done
15. PYTHON - LIST DATA STRUCUTRE	-	Done
16. PYTHON - TUPLE DATA STRUCUTRE	-	Done
17. PYTHON - SET DATA STRUCUTRE		-	Done
18. PYTHON - DICTIONARY DATA STRUCUTRE	-	Done
19. PYTHON - OBJECT ORIENTED 		-	Done
	PROGRAMMING	

--------------------------------------------------

DATA ANALYSIS				
-------------

1. PANDAS - INTRODUCTION		-	Done
2. PANDAS - SERIES - INTRODUCTION	-	Done
3. PANDAS - NAN VALUE			-	Done
4. PANDAS - SERIES - ATTRIBUTES		-	Done
5. PANDAS - SERIES - METHODS		-	Done
6. PANDAS - DATAFRAME INTRODUCTION	-	Done
7. PANDAS - DATAFRAME - LOADING 	-	Done
	DIFFERENT FILES

8. PANDAS - DATAFRAME - ATTRIBUTES	-	Done
9. PANDAS - DATAFRAME - METHODS		-	Done

10. PANDAS - DATAFRAME - RENAMING 	-	Done
	COLUMN, INDEX

11. PANDAS - DATAFRAME - INPLACE 	-	Done
	PARAMETER

12. PANDAS -DATAFRAME - HANDLING 	-	Done
	MISSING NAN VALUES

13. PANDAS - DATAFRAME - SELECTION 	- 	Done
	LOC, ILOC

14. PANDAS - DATAFRAME - FILTERING	-	Done

15. PANDAS - DATAFRAME - SORTING	-	Done

16. PANDAS - DATAFRAME - GROUPBY	-	Done

17. PANDAS - DATAFRAME - MERGING 	-	Done
	OR JOINING

18. PANDAS - DATAFRAME - CONCAT		-	Done

19. PANDAS - DATAFRAME - ADDING, 	-	Done
	DROPPING ROWS AND COLUMNS

20. PANDAS - DATAFRAME - DATE AND 	-	Done
	TIME OPERATIONS

21. PANDAS - DATAFRAME - CONCATENATING	-	Done 
	MULTIPLE CSV FILES

--------------------------------------------------

DATA ANALYSIS PROJECT
---------------------

1. EDA PROJECT				-	Done

--------------------------------------------------

DATA VISUALIZATION
------------------

1. DATA VISUALIZATION PART 1		-	Done
2. DATA VISUALIZATION PART 2		-	Done
3. DATA VISUALIZATION FUNDAMENTALS	-	Done

4. DATA VISUALIZATION POWER BI		-	Upcoming topic	
		
--------------------------------------------------

NUMPY
-----

1. NUMPY INTRODUCTION			-	Done
2. NUMPY FUNDAMENTALS			-	Done
3. NUMPY ATTRIBUTES			-	Done
4. NUMPY METHODS			-	Done

--------------------------------------------------

MATHS						STATUS
-----						------

1. MATHS - PART - 1 - POPULATION, 	-	Done
	SAMPLE, TYPES OF VARIABLES


2. MATHS - PART - 2 - MODE, MEDIAN, 	-	Done
	MEAN, RANGE, 
	STANDARD DEVIATION, VARIANCE	


3. MATHS - PART - 3 - OUTLIERS		-	Done


4. MATHS - PART - 4 - THE FIVE NUMBERS	-	Done
	SUMMARY, BOX PLOT, OUTLIER


5. MATHS - PART - 5 - SYMMETRY AND 	-	Done
	SKEWNESS


6. MATHS - PART - 6 - EXPLANATORY AND 	-	Done
	RESPONSIVE VARIABLES


7. MATHS - PART - 7 - REGRESSION 	-	Done
	AND R SQUARED


8. MATHS - PART - 8 - RESIDUALS		-	Done


9. MATHS - PART - 9 - THE NORMAL 	-	Hold
	DISTRIBUTION AND 
	68-95-99.7 RULE

10. MATHS - PART - 10 - MATRIX		-	Done

--------------------------------------------------

FEATURE ENGINEERING
-------------------

1. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 1	-	Done


2. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 2	-	Done

--------------------------------------------------

MACHINE LEARNING
----------------

1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	INTRODUCTION

2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TERMINOLOGY

3. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DATA AND ML ALGORITHMS

4. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LEARNING FUNCTION

5. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TYPES OF MODELS

6. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LIFE CYCLE

7. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TRAIN & TEST DATASETS


8. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	R VALUE


9. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION

9.1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

9.2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	MULTIPLE LINEAR REGRESSION


11. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	PICKLING AND UNPICKLING


12. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SAVE MODEL USING JOBLIB AND 
	PICKLING


13. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	POLYNOMIAL FEATURES

14. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DUMMY VARIABLE, ONEHOTENCODING


15. DATA SCIENCE - MACHINE LEARNING 	-	Done
	- R VALUE


16. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	COST FUNCTION


17. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	REGRESSION COST FUNCTION


18. DATA SCIENCE - MACHINE LEARNING 	- 	Done	
	LOGISTIC REGRESSION
	BINARY CLASSIFICATION

19. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LOGISTIC REGRESSION
	MULTI CLASS CLASSIFICATION


20. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DECISION TREE


21. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	RANDOM FOREST ALGORITHM



22. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	K-FOLD CROSS VALIDATION


23. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SVM

24. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	GRADIENT DESCENT ALGORITHM


25. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	UNDERFITTING & OVERFITTING


26. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	BIAS - VARIANCE TRADE OFF


27. DATA SCIENCE - MACHINE LEARNING 	- 	Running topic
	K - MEANS CLUSTERING


28. DATA SCIENCE - MACHINE LEARNING 	- 	Upcoming topic
	HYPER PARAMETER TUNING 
	GRIDSEARCH CV


--------------------------------------------------

Predefined functions
---------------------

1. print(p)		->	To display the output
2. type(p)		->	To check the data type
3. range(p)		->	To get range of values
4. input(p)		->	To take valut at runtime/dynamically
5. len(p)		->	To find number of values in sequence

6. float(p)		->	To convert to float
7. int(p)		->	To convert to int
8. list(p)		->	Convert from seq to list
9. tuple(p)		->	Convert from seq to tuple
10. set(p)		->	Convert from seq to set

11. dict(p)		->	Convert from list of tups to dict

--------------------------------

Errors
------

1. SyntaxError
2. NameError
3. KeyError
4. ValueError
5. TypeError

6. IndexError
7. IndentationError
8. AttributeError
9. ModuleNotFoundError
10. FileNotFoundError

11. InvalidParameterError

------------------------------------

27. K - MEANS CLUSTERING - Continue

28. DATA SCIENCE - MACHINE LEARNING 	
	HYPER PARAMETER TUNING 
	GRIDSEARCH CV

------------------------------------

Imp ponits!!!
-------------

	1. ML Flow
	2. ML Steps

------------------------------------

ML flow
-------

Data
	DataFrame
		Feature Engg
				Array
					Machine Learning Algorithm
					Cost function
					Gradient Descent Algorithm
						Increase accuracy
						Reduce Error
							Bias
							Variance


ML steps
--------

	1. Importing the libraries
	2. Loading the dataset
	3. Data preparation
	4. Splitting the dataset
	5. Model creation
	6. Model training
	7. Prediction

--------------------------------

*** K - MEANS CLUSTERING ***
----------------------------

# Elbow method technique

print("Topic: Elbow method")
print()




print("Step 1: Importing the libraries")

import pandas as pd
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler





print("Step 2: Loading the dataset")

df = pd.read_csv("income.csv")




print("Step 3: Data preparation")

scaler = MinMaxScaler()

scaler.fit(df[['Age']])
df['Age'] = scaler.transform(df[['Age']])

scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])


sse = []

k_rng = range(1, 10)

for k in k_rng:
    km = KMeans(n_clusters = k)
    km.fit(df[['Age', 'Income($)']])
    
    sse.append(km.inertia_)
    
plt.xlabel('K')
plt.ylabel('Sum of squared error')

plt.plot(k_rng, sse)

plt.show()

--------------------------------

31. DATA SCIENCE - MACHINE LEARNING - 
	HYPER PARAMETER TUNING
	GRIDSEARCH CV

--------------------------------

How to improve model performance?

	1. train_test_split()
	2. K fold cross validation
	3. Hyper parameter tuning technique
		1. GridSearchCV
		2. RandomizedSearchCV

--------------------------------

3. Hyper parameter tuning technique

Machine learning Alg

-> Having two types of parameters

	1. Normal/Default parameters	->	85
	2. Hyper parameters		->	92


--------------------------------

-> Schooling 10th class
-> Entered into 10th class
-> 3 months:
	Progress card Time
	Father: 35/100

-> Get ready: Monday we will go to one place
-> Where & why?
	Tuition
	To improve your studies
	Joined in tuition

-> 5 months:
	Progress card time:
	Father: 60/100

--------------------------------

1. Default parameters
	model = SVC()
	model = RandomForestClassifier()


2. Hyper parameter
	model = SVC(hyper parameters...)
	model = RandomForestClassifier(n_estimator = 40)

--------------------------------

-> How to apply Hyper parameters
-> We can apply by using two ways

	1. GridSearchCV
	2. RandomizedSearchCV

--------------------------------

from Masud Siddiqi (privately):    6:05 PM
then what parameters ML models learn?

-> We will see while working, today itself :)

--------------------------------

iris dataset:
-------------

	1. train_test_split
	2. K fold cross validation
	3. Hyper parameters using GridSearchCV

--------------------------------

1. train_test_split
-------------------

print("Topic: Hyper parameter tuning: train_test_split")
print()




print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split




print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target





print("Step 4: Splitting the dataset")

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size = 0.3
)




print("Step 5: Model creation")

model = svm.SVC(kernel = 'rbf', C = 30, gamma = 'auto')





print("Step 6: Model training")

model.fit(X_train, y_train)



print("Step Spl: Check the score")

print()
print(model.score(X_test, y_test))

output
------

C:\Users\Daniel\Desktop\CODING>py demo1.py
Topic: Hyper parameter tuning: train_test_split

Step 1: Importing the libraries
Step 2: Loading the dataset
Step 3: Data preparation
Step 4: Splitting the dataset
Step 5: Model creation
Step 6: Model training
Step Spl: Check the score

0.9333333333333333

--------------------------------

Topic: Hyper parameter tuning: K fold cross validataion

Combintation 1:
--------------
	kernel = 'linear', 
	C = 10, 
	gamma = 'auto'

print("Topic: Hyper parameter tuning: K fold cross validataion")
print()




print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import cross_val_score





print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target





print("Step 4: Splitting the dataset: K fold cross val")




print("Step 5: Model creation")

model1 = svm.SVC(
    kernel = 'linear', 
    C = 10, 
    gamma = 'auto'
)


scores = cross_val_score(model1, X, y, cv = 5)

print()
print(scores)


--------------------------------

Topic: Hyper parameter tuning: K fold cross validataion

Combintation 2:
--------------
	kernel = 'rbf', 
	C = 10, 
	gamma = 'auto'

print("Topic: Hyper parameter tuning: K fold cross validataion")
print()




print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import cross_val_score





print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target





print("Step 4: Splitting the dataset: K fold cross val")




print("Step 5: Model creation")

model1 = svm.SVC(
    kernel = 'rbf', 
    C = 10, 
    gamma = 'auto'
)


scores = cross_val_score(model1, X, y, cv = 5)

print()
print(scores)

--------------------------------

Topic: Hyper parameter tuning: K fold cross validataion

Combintation 3:
--------------
	kernel = 'rbf', 
	C = 20, 
	gamma = 'auto'


print("Topic: Hyper parameter tuning: K fold cross validataion")
print()




print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import cross_val_score





print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target





print("Step 4: Splitting the dataset: K fold cross val")




print("Step 5: Model creation")

model1 = svm.SVC(
    kernel = 'rbf', 
    C = 20, 
    gamma = 'auto'
)


scores = cross_val_score(model1, X, y, cv = 5)

print()
print(scores)

--------------------------------

Hyper parameters tuning technique

	1. train_test_split	Run 3 times
	2. K fold cross validataion
		combination1
		combination2
		combination3
		etc...

	3. GridSearchCV
		
--------------------------------

GridSearchCV 
------------

print("Hyper parameter tuning: GridSearchCV")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV



print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target



d = {
    'C' : [1, 10, 20],
    'kernel' :['rbf', 'linear']
}




print("Step 4: Splitting the dataset: Opt")

print("Step 5: Model creation")

model = svm.SVC(gamma = 'auto')



print("Step Spl1: Use GridSearchCV")

obj = GridSearchCV(
    model,
    d,
    cv = 5,
    return_train_score = False
)


print("Step Spl2: Use GridSearchCV")

obj.fit(X, y)

print()
print("Checking the result")
print()
df = pd.DataFrame(obj.cv_results_)
print(df)

--------------------------------

GridSearchCV Results

print("Hyper parameter tuning: GridSearchCV")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV



print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target



d = {
    'C' : [1, 10, 20],
    'kernel' :['rbf', 'linear']
}




print("Step 4: Splitting the dataset: Opt")

print("Step 5: Model creation")

model = svm.SVC(gamma = 'auto')



print("Step Spl1: Use GridSearchCV")

obj = GridSearchCV(
    model,
    d,
    cv = 5,
    return_train_score = False
)


print("Step Spl2: Use GridSearchCV")

obj.fit(X, y)

print()
print("Checking the result")
print()
df = pd.DataFrame(obj.cv_results_)

result = df[['param_C', 'param_kernel', 'mean_test_score']]
print(result)

--------------------------------

# GridSearchCV: Best parameter values

print("Hyper parameter tuning: GridSearchCV")
print()


print("Step 1: Importing the libraries")

import pandas as pd
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV



print("Step 2: Loading the dataset")

iris = datasets.load_iris()




print("Step 3: Data preparation")

X = iris.data
y = iris.target



d = {
    'C' : [1, 10, 20],
    'kernel' :['rbf', 'linear']
}




print("Step 4: Splitting the dataset: Opt")

print("Step 5: Model creation")

model = svm.SVC(gamma = 'auto')



print("Step Spl1: Use GridSearchCV")

obj = GridSearchCV(
    model,
    d,
    cv = 5,
    return_train_score = False
)


print("Step Spl2: Use GridSearchCV")

obj.fit(X, y)

print()
print("Checking the result")
print()
df = pd.DataFrame(obj.cv_results_)

result = df[['param_C', 'param_kernel', 'mean_test_score']]
print(result)

print()
print(obj.best_params_)

--------------------------------

-> I will continue in next session
			  - Daniel

--------------------------------

Daily
-----

1. Running notes				->	Sharing
2. Materials (PDF format)			->	Sharing

We are sharing by using 			->	Google classroom

--------------------------------
