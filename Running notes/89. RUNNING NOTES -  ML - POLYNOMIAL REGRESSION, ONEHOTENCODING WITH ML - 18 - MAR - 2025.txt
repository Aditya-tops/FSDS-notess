RUNNING NOTES: 18 - MAR - 2025
--------------------------------

1. PREVIOUS TOPIC			-	ML: SAVE THE MODEL
2. CURRENT TOPIC			-	ML: POLYNOMIAL FEATURE
3. UPCOMING TOPIC			-	ML: ONEHOTENCODING + ML
									         

----------------------------------------------------------------


INDEX
---------

0. DATA SCIENCE DEMO			-	Done

1. DATA SCIENCE FUNDAMENTALS		-	Done

--------------------------------------------------

PYTHON PROGRAMMING LANG
-----------------------

0. PYTHON - INSTALLATION		-	Done

1. PYTHON - INTRODUCTION		-	Done
2. PYTHON - KEYWORDS			-	Done
3. PYTHON - HELLO WORLD PROGRAM		-	Done
4. PYTHON - NAMING CONVENTIONS		-	Done
5. PYTHON - VARIABLES			-	Done
6. PYTHON - DATA TYPES			-	Done
7. PYTHON - OPERATORS			-	Done
8. PYTHON - INPUT & OUTPUT		-	Done
9. PYTHON - FLOW CONTROL		-	Done
10. PYTHON - STRING			-	Done
11. PYTHON - FUNCTIONS - PART - 1	-	Done
12. PYTHON - FUNCTIONS - PART - 2	-	Done
13. PYTHON - MODULE			-	Done
14. PYTHON - PACKAGE			-	Done
15. PYTHON - LIST DATA STRUCUTRE	-	Done
16. PYTHON - TUPLE DATA STRUCUTRE	-	Done
17. PYTHON - SET DATA STRUCUTRE		-	Done
18. PYTHON - DICTIONARY DATA STRUCUTRE	-	Done
19. PYTHON - OBJECT ORIENTED 		-	Done
	PROGRAMMING	

--------------------------------------------------

DATA ANALYSIS				
-------------

1. PANDAS - INTRODUCTION		-	Done
2. PANDAS - SERIES - INTRODUCTION	-	Done
3. PANDAS - NAN VALUE			-	Done
4. PANDAS - SERIES - ATTRIBUTES		-	Done
5. PANDAS - SERIES - METHODS		-	Done
6. PANDAS - DATAFRAME INTRODUCTION	-	Done
7. PANDAS - DATAFRAME - LOADING 	-	Done
	DIFFERENT FILES

8. PANDAS - DATAFRAME - ATTRIBUTES	-	Done
9. PANDAS - DATAFRAME - METHODS		-	Done

10. PANDAS - DATAFRAME - RENAMING 	-	Done
	COLUMN, INDEX

11. PANDAS - DATAFRAME - INPLACE 	-	Done
	PARAMETER

12. PANDAS -DATAFRAME - HANDLING 	-	Done
	MISSING NAN VALUES

13. PANDAS - DATAFRAME - SELECTION 	- 	Done
	LOC, ILOC

14. PANDAS - DATAFRAME - FILTERING	-	Done

15. PANDAS - DATAFRAME - SORTING	-	Done

16. PANDAS - DATAFRAME - GROUPBY	-	Done

17. PANDAS - DATAFRAME - MERGING 	-	Done
	OR JOINING

18. PANDAS - DATAFRAME - CONCAT		-	Done

19. PANDAS - DATAFRAME - ADDING, 	-	Done
	DROPPING ROWS AND COLUMNS

20. PANDAS - DATAFRAME - DATE AND 	-	Done
	TIME OPERATIONS

21. PANDAS - DATAFRAME - CONCATENATING	-	Done 
	MULTIPLE CSV FILES

--------------------------------------------------

DATA ANALYSIS PROJECT
---------------------

1. EDA PROJECT				-	Done

--------------------------------------------------

DATA VISUALIZATION
------------------

1. DATA VISUALIZATION PART 1		-	Done
2. DATA VISUALIZATION PART 2		-	Done
3. DATA VISUALIZATION FUNDAMENTALS	-	Done

4. DATA VISUALIZATION POWER BI		-	Upcoming topic	
		
--------------------------------------------------

NUMPY
-----

1. NUMPY INTRODUCTION			-	Done
2. NUMPY FUNDAMENTALS			-	Done
3. NUMPY ATTRIBUTES			-	Done
4. NUMPY METHODS			-	Done

--------------------------------------------------

MATHS						STATUS
-----						------

1. MATHS - PART - 1 - POPULATION, 	-	Done
	SAMPLE, TYPES OF VARIABLES


2. MATHS - PART - 2 - MODE, MEDIAN, 	-	Done
	MEAN, RANGE, 
	STANDARD DEVIATION, VARIANCE	


3. MATHS - PART - 3 - OUTLIERS		-	Done


4. MATHS - PART - 4 - THE FIVE NUMBERS	-	Done
	SUMMARY, BOX PLOT, OUTLIER


5. MATHS - PART - 5 - SYMMETRY AND 	-	Done
	SKEWNESS


6. MATHS - PART - 6 - EXPLANATORY AND 	-	Done
	RESPONSIVE VARIABLES


7. MATHS - PART - 7 - REGRESSION 	-	Done
	AND R SQUARED


8. MATHS - PART - 8 - RESIDUALS		-	Done


9. MATHS - PART - 9 - THE NORMAL 	-	Hold
	DISTRIBUTION AND 
	68-95-99.7 RULE

10. MATHS - PART - 10 - MATRIX		-	Done

--------------------------------------------------

FEATURE ENGINEERING
-------------------

1. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 1	-	Done


2. FEATURE ENGINEERING - 
	DATA PRE PROCESSING - PART - 2	-	Done

--------------------------------------------------

MACHINE LEARNING
----------------

1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	INTRODUCTION

2. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TERMINOLOGY

3. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	DATA AND ML ALGORITHMS

4. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LEARNING FUNCTION

5. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TYPES OF MODELS

6. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	LIFE CYCLE

7. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	TRAIN & TEST DATASETS


8. DATA SCIENCE - MACHINE LEARNING 	- 	Upcoming topic	
	R VALUE


9. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION

9.1. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

9.2. DATA SCIENCE - MACHINE LEARNING 	- 	Share
	SIMPLE LINEAR REGRESSION 
	EXAMPLE

10. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	MULTIPLE LINEAR REGRESSION


12. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	PICKLING AND UNPICKLING


13. DATA SCIENCE - MACHINE LEARNING 	- 	Done
	SAVE MODEL USING JOBLIB AND 
	PICKLING


10. DATA SCIENCE - MACHINE LEARNING 	- 	Running topic
	POLYNOMIAL FEATURES


--------------------------------------------------

Predefined functions
---------------------

1. print(p)		->	To display the output
2. type(p)		->	To check the data type
3. range(p)		->	To get range of values
4. input(p)		->	To take valut at runtime/dynamically
5. len(p)		->	To find number of values in sequence

6. float(p)		->	To convert to float
7. int(p)		->	To convert to int
8. list(p)		->	Convert from seq to list
9. tuple(p)		->	Convert from seq to tuple
10. set(p)		->	Convert from seq to set

11. dict(p)		->	Convert from list of tups to dict

--------------------------------

Errors
------

1. SyntaxError
2. NameError
3. KeyError
4. ValueError
5. TypeError

6. IndexError
7. IndentationError
8. AttributeError
9. ModuleNotFoundError
10. FileNotFoundError

11. InvalidParameterError

--------------------------------

10. DATA SCIENCE - MACHINE LEARNING
	POLYNOMIAL FEATURES

--------------------------------

ML flow
-------

Data
	DataFrame
		Feature Engg
				Array
					Machine Learning Algorithm
					Cost function
					Gradient Descent Algorithm
						Increase accuracy
						Reduce Error
							Bias
							Variance


ML steps
--------

	1. Importing the libraries
	2. Loading the dataset
	3. Data preparation
	4. Splitting the dataset
	5. Model creation
	6. Model training
	7. Prediction

--------------------------------

Data
----
	1. Linear
	2. Non-linear

--------------------------------

Simple Linear Regression

	..
	model = LinearRegression()
	model.fit(feature, target)
	model.fit(X, y)

Polynomial Regression

	..
	model = LinearRegression()
	model.fit(polynomial features, target)
	model.fit(X_poly, y)


--------------------------------

# Plotting the dataset

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training")

model.fit(X, y)


print("Step Spl: Data visualization")

plt.scatter(X, y, color = "blue")

plt.plot(X, model.predict(X), color = "red")


plt.title("Linear Regression")
plt.xlabel("Position Levels")
plt.ylabel("Salary")

plt.show()

--------------------------------

# Polynimial Regression
	1. Create a polynomial features
	2. Train the model with polynomial features
		(Called as Polynomial Regression)


--------------------------------

With LinearRegression prediction value is:

Level	Salary
1	45000
2	50000
3	60000
4	80000
5	110000
6	150000

6.5		1,50,000		2,00,000

7	200000
8	300000
9	500000
10	1000000

model prediction is:	3,30,000	wrong prediction


--------------------------------

# Creating polynomial feature with degree value is 1

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 1)

x_poly = poly_regs.fit_transform(X)


print()
print(X)
print()
print(x_poly)


--------------------------------

# Creating polynomial feature with degree value is 2

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 2)

x_poly = poly_regs.fit_transform(X)


print()
print(X)
print()
print(x_poly)

--------------------------------

# Polynomial Regression

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 2)

x_poly = poly_regs.fit_transform(X)





print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training with polynomial features")

model.fit(x_poly, y)


--------------------------------

# Polynomial features, degree = 1

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 1)

x_poly = poly_regs.fit_transform(X)





print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training with polynomial features")

model.fit(x_poly, y)





print("Step Spl2: Data Viz")

plt.scatter(X, y, color = "blue")

plt.plot(X, model.predict(x_poly), color="red")

plt.title("Polynomial Regression")

plt.xlabel("Position Levels")
plt.ylabel("Salary")

plt.show()


--------------------------------

# Polynomial features, degree = 2

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 2)

x_poly = poly_regs.fit_transform(X)





print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training with polynomial features")

model.fit(x_poly, y)





print("Step Spl2: Data Viz")

plt.scatter(X, y, color = "blue")

plt.plot(X, model.predict(x_poly), color="red")

plt.title("Polynomial Regression")

plt.xlabel("Position Levels")
plt.ylabel("Salary")

plt.show()

--------------------------------

# Polynomial features, degree = 3

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 3)

x_poly = poly_regs.fit_transform(X)





print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training with polynomial features")

model.fit(x_poly, y)





print("Step Spl2: Data Viz")

plt.scatter(X, y, color = "blue")

plt.plot(X, model.predict(x_poly), color="red")

plt.title("Polynomial Regression")

plt.xlabel("Position Levels")
plt.ylabel("Salary")

plt.show()


--------------------------------

# Polynomial features, degree = 4

print("Topic: Polynomial Regression")
print()


print("Step 1: Importing the libraires")

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures



print("Step 2: Loading the dataset")

df = pd.read_csv("poly_dataset.csv")




print("Step 3: Data preparation: X, y")

X = df.iloc[:, 1:2].values
y = df.iloc[:, 2].values




print("Step Spl1: Creating polynomial Features")

poly_regs = PolynomialFeatures(degree = 4)

x_poly = poly_regs.fit_transform(X)





print("Step 4: Splitting the dataset: Opt")


print("Step 5: Model creation")

model = LinearRegression()




print("Step 6: Model training with polynomial features")

model.fit(x_poly, y)





print("Step Spl2: Data Viz")

plt.scatter(X, y, color = "blue")

plt.plot(X, model.predict(x_poly), color="red")

plt.title("Polynomial Regression")

plt.xlabel("Position Levels")
plt.ylabel("Salary")

plt.show()

--------------------------------

Question:

Daniel what is the best value for degree,

Can I give 10 value?

-> We need to check by using data visualization
-> Best fitted line:
	The line which is by the data points

-> If we give degree = 10:
	Diagram looks pretty good but that is not a good value
	This model is overfitting model

--------------------------------

Good question!!!

Diff between fit() and fit_trasform()

-> fit()	belongs to Machine learing:	Training
-> fit_transform() beongs to Featunre Engg:	creating features

--------------------------------

from Nageswara Rao Anumolu (privately):    6:13 PM
Sir, sorry if this question is not valid now. We can have this visualization for 2 dimension data. If the data is having more dimensions, how to decide this degree value, (as we may not be able to visualize the data with those many dimensions)


Points!!!

2 dimention data	->	Create a Line
3 dimention data	->	Create a plane(SVM)

--------------------------------

Prediction
----------

With Polynomial Regression prediction value is:

Level	Salary
1	45000
2	50000
3	60000
4	80000
5	110000
6	150000

6.5		1,50,000		2,00,000
			   1,58,862
7	200000
8	300000
9	500000
10	1000000

model prediction is:	1,58,862	right prediction

--------------------------------

from Charan GMS (privately):    6:23 PM
sir, fit is already done fit for polyfeatures then why again?? what will happen if we apply only transform ??


from ...MS (privately):    6:26 PM
fit is done for polyfeatures sir

from ...GMS (privately):    6:27 PM
and while predicting why again fit?

Sorry for saying: others didnt understand the topic

Be clear about, Feature Engg, Model creation


fit_transform()	->	Featuren Engg tech
		->	Its not training purpose

fit()		->	not feature engg techn
		->	training purpose

--------------------------------

# Just method name is matching but functionality is different

python
	count	str	list	tuple

pandas
	count	Series	DataFrame


Feature Engg
	fit_transform()		method

Machine learning
	fit()			method


--------------------------------

Helpful!!!
----------

If you done 10 times revision then go for interview


If you done 1 time revison, I request you even if you attend interview literally you will fail interview

Confusion, fear	->	failed

Conclusion	->	Do more practice

--------------------------------

Vij		1
Guntur		2
Gudivada	3

1 + 1		2

Vij + Vij	Guntur

--------------------------------

5. Dummy variables

-> We done today session
-> We will connect again tomorrow
-> Daniel :)

--------------------------------

Daily
-----

1. Running notes				->	Sharing
2. Materials (PDF format)			->	Sharing

We are sharing by using 			->	Google classroom

--------------------------------
